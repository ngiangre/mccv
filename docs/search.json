[
  {
    "objectID": "find_confounding.html",
    "href": "find_confounding.html",
    "title": "Identifying Confounders",
    "section": "",
    "text": "One of the many subtasks when performing a prediction scheme is adjusting for mediating and confounding variables. But what is a mediator, what is a confounder, and why include thhem? Counfounder and Mediators can be illustrated using a causal diagram (From this Wikipedia page):\n\nAs you can see, a confounding variable is something that confounds or ‘confuses’ the relationship between an exposure and an outcome. The relationship between an exposure X and an outcome Y is influenced by a confounding variable Z. A mediating variable connects an exposure to an outcome where otherwise would not be. The relationship between an exposure X and an outcome Y is influenced by a mediating variable Z.\nHere, I will show how MCCV can be used to identify confounders and mediators.\n\nConfoundingMediation\n\n\nGenerate dataset with an effect X, outcome Y, and a confounder Z variables\nThe random variable Z generates X and Y. Any correlation between X and Y is therefore spurious\n\n\nShow The Code\n\nimport numpy as np\nN=100\nZ1 = np.random.normal(loc=0,scale=1,size=N)\nZ2 = np.random.normal(loc=1,scale=1,size=N)\nZ = np.concatenate([Z1,Z2])\n\nimport scipy as sc\nY = np.concatenate([sc.stats.bernoulli.rvs(z,size=1) for z in (Z - min(Z)) / (max(Z) - min(Z))])\nX = Z + np.random.normal(loc=0,scale=1,size=len(Z))\n\n\n\n\nShow The Code\ndf <- tibble::tibble(\n    X = reticulate::py$X,\n    Y = reticulate::py$Y,\n    Z = reticulate::py$Z\n)\ndf[['Y']] <- factor(df$Y,levels=c(0,1))\n\ndf\n\n\n# A tibble: 200 × 3\n           X Y             Z\n   <dbl[1d]> <fct> <dbl[1d]>\n 1   -0.861  0       -0.0394\n 2   -0.0157 0       -0.649 \n 3    0.857  0        0.626 \n 4    1.78   0        1.84  \n 5   -1.33   0       -0.539 \n 6    0.688  0       -1.40  \n 7   -0.132  0        0.949 \n 8    0.636  0       -0.702 \n 9    1.43   0        0.606 \n10   -0.276  0       -0.319 \n# … with 190 more rows\n\n\nShow The Code\nGGally::ggpairs(df)\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEmploy MCCV: Predict Y from X, Y from Z, Y from X and Z\n\n\nShow The Code\nimport pandas as pd\ndf = pd.DataFrame(data={'X' : X,'Y' : Y,'Z' : Z})\ndf.index.name = 'pt'\n\nX = df.loc[:,['X']]\nY = df.loc[:,['Y']]\nZ = df.loc[:,['Z']]\nXZ = df.loc[:,['X','Z']]\n\n\n\n\nShow The Code\nimport mccv\n\nmccv_YXobj = mccv.mccv(num_bootstraps=200,n_jobs=2)\nmccv_YXobj.set_X(X)\nmccv_YXobj.set_Y(Y)\nmccv_YXobj.run_mccv()\n\nmccv_YZobj = mccv.mccv(num_bootstraps=200,n_jobs=2)\nmccv_YZobj.set_X(Z)\nmccv_YZobj.set_Y(Y)\nmccv_YZobj.run_mccv()\n\nmccv_YXZobj = mccv.mccv(num_bootstraps=200,n_jobs=2)\nmccv_YXZobj.set_X(XZ)\nmccv_YXZobj.set_Y(Y)\nmccv_YXZobj.run_mccv()\n\n\n\n\nShow The Code\n\nf_imp_dfs = dict()\nf_imp_dfs['YXobj'] = \\\nmccv_YXobj.mccv_data['Feature Importance']\nf_imp_dfs['YZobj'] = \\\nmccv_YZobj.mccv_data['Feature Importance']\nf_imp_dfs['YXZobj'] = \\\nmccv_YXZobj.mccv_data['Feature Importance']\n\n\nVisualize feature importances\n\n\nShow The Code\nlibrary(ggplot2)\n\nf_imp_plot <- function(x,title){\n    ggplot(x,aes(feature,importance,color=feature)) +\n    geom_boxplot(alpha=0) +\n    geom_point(position=position_jitter(width=0.2)) +\n    scale_color_manual(values=c(\"X\" = \"indianred\",\n                                \"Y\" = \"skyblue\",\n                                \"Z\" = \"black\",\n                                \"Intercept\" = \"gray\")) +\n    theme_bw() +\n    labs(title=title)\n}\n\nlibrary(patchwork)\n(f_imp_plot( reticulate::py$f_imp_dfs$YXobj,\"Y ~ X\" ) +\n    f_imp_plot( reticulate::py$f_imp_dfs$YZobj,\"Y ~ Z\" ) ) /\n    f_imp_plot( reticulate::py$f_imp_dfs$YXZobj,\"Y ~ X + Z\" )\n\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\n\n\n\n\nIn this example, Z is confounding the relationship between X and Y. The ‘Y ~ X’ model shows that X is very important in predicting Y. However, when Z is included in the model ‘Y ~ X + Z’, you see that Z remains very important but X is no longer important for predicting Y. This toy example had Z causing Y and Z causing X, and any relationship between X and Y was spurious. Therefore, including X and Z as predictors showed only the importance of Z in predicting Y.\n\n\nGenerate dataset with an effect X, outcome Y, and a mediator Z variables\nThe random variable X generates Z which generates and Y. Any correlation between X and Y is therefore spurious\n\n\nShow The Code\n\nimport numpy as np\nN=100\nX1 = np.random.normal(loc=0,scale=1,size=N)\nX2 = np.random.normal(loc=1,scale=1,size=N)\nX = np.concatenate([X1,X2])\n\nimport scipy as sc\nZ = X + np.random.normal(loc=0,scale=1,size=len(X))\nY = np.concatenate([sc.stats.bernoulli.rvs(z,size=1) for z in (Z - min(Z)) / (max(Z) - min(Z))])\n\n\n\n\nShow The Code\ndf <- tibble::tibble(\n    X = reticulate::py$X,\n    Y = reticulate::py$Y,\n    Z = reticulate::py$Z\n)\ndf[['Y']] <- factor(df$Y,levels=c(0,1))\n\ndf\n\n\n# A tibble: 200 × 3\n           X Y             Z\n   <dbl[1d]> <fct> <dbl[1d]>\n 1    -0.566 1        -1.13 \n 2     0.728 0         0.519\n 3     0.456 0        -0.657\n 4    -0.833 0        -0.908\n 5    -0.199 1         0.225\n 6    -0.241 0        -2.03 \n 7     0.322 0        -0.649\n 8    -2.55  0        -2.90 \n 9     0.350 0        -1.17 \n10     0.563 0        -0.117\n# … with 190 more rows\n\n\nShow The Code\nGGally::ggpairs(df)\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEmploy MCCV: Predict Y from X, Y from Z, Y from X and Z\n\n\nShow The Code\nimport pandas as pd\ndf = pd.DataFrame(data={'X' : X,'Y' : Y,'Z' : Z})\ndf.index.name = 'pt'\n\nX = df.loc[:,['X']]\nY = df.loc[:,['Y']]\nZ = df.loc[:,['Z']]\nXZ = df.loc[:,['X','Z']]\n\n\n\n\nShow The Code\nimport mccv\n\nmccv_YXobj = mccv.mccv(num_bootstraps=200,n_jobs=2)\nmccv_YXobj.set_X(X)\nmccv_YXobj.set_Y(Y)\nmccv_YXobj.run_mccv()\n\nmccv_YZobj = mccv.mccv(num_bootstraps=200,n_jobs=2)\nmccv_YZobj.set_X(Z)\nmccv_YZobj.set_Y(Y)\nmccv_YZobj.run_mccv()\n\nmccv_YXZobj = mccv.mccv(num_bootstraps=200,n_jobs=2)\nmccv_YXZobj.set_X(XZ)\nmccv_YXZobj.set_Y(Y)\nmccv_YXZobj.run_mccv()\n\n\n\n\nShow The Code\n\nf_imp_dfs = dict()\nf_imp_dfs['YXobj'] = \\\nmccv_YXobj.mccv_data['Feature Importance']\nf_imp_dfs['YZobj'] = \\\nmccv_YZobj.mccv_data['Feature Importance']\nf_imp_dfs['YXZobj'] = \\\nmccv_YXZobj.mccv_data['Feature Importance']\n\n\nVisualize feature importances\n\n\nShow The Code\nlibrary(ggplot2)\n\nf_imp_plot <- function(x,title){\n    ggplot(x,aes(feature,importance,color=feature)) +\n    geom_boxplot(alpha=0) +\n    geom_point(position=position_jitter(width=0.2)) +\n    scale_color_manual(values=c(\"X\" = \"indianred\",\n                                \"Y\" = \"skyblue\",\n                                \"Z\" = \"black\",\n                                \"Intercept\" = \"gray\")) +\n    theme_bw() +\n    labs(title=title)\n}\n\nlibrary(patchwork)\n(f_imp_plot( reticulate::py$f_imp_dfs$YXobj,\"Y ~ X\" ) +\n    f_imp_plot( reticulate::py$f_imp_dfs$YZobj,\"Y ~ Z\" ) ) /\n    f_imp_plot( reticulate::py$f_imp_dfs$YXZobj,\"Y ~ X + Z\" )\n\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\n\n\n\n\nIn this example, Z is a mediating relationship between X and Y. The ‘Y ~ X’ model shows that X is very important in predicting Y. However, when Z is included in the model ‘Y ~ X + Z’, you see that Z remains very important but X is no longer important for predicting Y. This toy example had Z causing Y and X relating to Y through Z, and any relationship between X and Y was mediated through Z. Therefore, including X and Z as predictors showed only the importance of Z in predicting Y."
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "We start out with a dataset where we want to evaluate an outcome, Y, to be predicted by an independent variable, X. In this example, X determines Y and so we expect Y to be predicted by X to some degree. MCCV can be used to quantify that prediction. We’ll refer to the observations as patients, as you can view the dataset applying to patients experiencing an outcome during or at the end of a clinical trial.\nBelow, I’m demonstrating how I would go between Python and R, but you can stay solely in R or Python.\n\nGenerate a Dataset: Binary outcome Y and independent variable(s) X\n\n\nShow The Code\n\nimport numpy as np\nN=100\nX1 = np.random.normal(loc=0,scale=1,size=N)\nX2 = np.random.normal(loc=1,scale=1,size=N)\nX = np.concatenate([X1,X2])\nY = np.concatenate([np.repeat(0,N),np.repeat(1,N)])\n\n\n\n\nShow The Code\ndf <- \n    tibble::tibble(\n        X = reticulate::py$X,\n        Y = reticulate::py$Y\n        )\ndf[['Y']] <- factor(df$Y,levels=c(0,1))\n\nlibrary(ggplot2)\nggplot(data=df,mapping=aes(Y,X,color=Y)) +\n    geom_boxplot(alpha=0) +\n    geom_point(position=position_jitter(width=0.2)) +\n    scale_color_brewer(palette=\"Set1\") +\n    theme_bw()\n\n\n\n\n\n\n\nEmploy MCCV: Predict Y from X\n\n\nShow The Code\nimport pandas as pd\ndf = pd.DataFrame(data={'X' : X,'Y' : Y})\ndf.index.name = 'pt'\n\nX = df.loc[:,['X']]\nY = df.loc[:,['Y']]\n\n\n\n\nShow The Code\nimport mccv\nmccv_obj = mccv.mccv(num_bootstraps=200,n_jobs=2)\nmccv_obj.set_X(X)\nmccv_obj.set_Y(Y)\nmccv_obj.run_mccv()\n\n\n\n\nShow The Code\nml_df = mccv_obj.mccv_data['Model Learning']\nf_imp_df = mccv_obj.mccv_data['Feature Importance']\npreds_df = mccv_obj.mccv_data['Performance']\npt_preds_df = (\n    mccv_obj.mccv_data['Patient Predictions'].\n    reset_index()\n    )\n\n\n\n\nVisualize prediction results\n\n\nShow The Code\nlibrary(magrittr)\np1 <- reticulate::py$ml_df %>% \n    tidyr::pivot_longer(\n        cols = dplyr::contains('roc')\n    ) %>% \n    dplyr::mutate(\n        name = factor(name,levels=c(\n            \"train_roc_auc\",\"test_roc_auc\",\n            \"validation_roc_auc\"\n        ))\n    ) %>% \n    ggplot(aes(model,value,color=name)) +\n    geom_boxplot(alpha=0) +\n    geom_point(aes(group=name),\n               position=position_jitterdodge(\n                   dodge.width = 0.7,\n                   jitter.width = 0.1\n                   )\n               ) +\n    scale_color_brewer(palette=\"Set1\") +\n    scale_y_continuous(limits=c(0,1)) +\n    theme_bw() +\n    labs(title=\"Model performance results\")\np2 <- reticulate::py$f_imp_df %>% \n    ggplot(aes(feature,importance,color=feature)) +\n    geom_boxplot(alpha=0) +\n    geom_point(position=position_jitter(width=0.2)) +\n    scale_color_brewer(palette=\"Set2\") +\n    theme_bw() +\n    labs(title=\"Model feature importance results\")\n\n\nWarning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated\nvalues: row names not set\n\n\nShow The Code\np3 <- reticulate::py$preds_df %>% \n    ggplot(aes(metric,value)) +\n    geom_boxplot(alpha=0) +\n    geom_point(pch=21,\n               position=position_jitter(width=0.2)) +\n    theme_bw() +\n    labs(title=\"Model validation predictions\",\n         subtitle=\"From bootstrap of patient probabilities\")\np4 <- reticulate::py$pt_preds_df %>% \n    dplyr::mutate(\n        y_true = factor(y_true,levels=c(0,1))\n    ) %>% \n    dplyr::group_by(bootstrap,model,y_true) %>% \n    dplyr::summarise(\n        mean_y_proba = mean(y_proba),\n        .groups = \"drop\"\n    ) %>% \n    ggplot(aes(y_true,mean_y_proba,color=y_true)) +\n    geom_boxplot(alpha=0) +\n    geom_point(position=position_jitter(width=0.2)) +\n    scale_color_manual(values=c(\"orange\",\"purple\")) +\n    facet_wrap(~model) +\n    scale_y_continuous(limits=c(0,1)) +\n    theme_bw() +\n    labs(title=\"Patient class predictions\",\n         subtitle=\"Average of bootstrap probabilities per patient\")\n\nlibrary(patchwork)\np1 + p2 + p3 + p4 + plot_layout(ncol = 2)\n\n\n\n\n\nThese plots are an overview to the main results generated by this algorithm:\n\nThe performance of the model, across the bootstraps, during training and testing (within 10 fold cross validation) and also validation on the unseen patients.\nThe coefficients or importances quantified by the model fit on the entire training (train+test from above) dataset after the 10-fold cross validation for hyperparameter tuning.\nThe model validation predictions derived by bootstrapping (N=50) the resulting patient probabilities.\nThe predicted patient probabilities compared to the true patient classes or outcomes. Here, the probabilities were averaged per patient across the bootstraps.\n\n\n\nInterpreting MCCV predictions\nSome observations from this small example:\n\nAs expected, the testing performance is much higher than the model training performance. The model validation performance has more variance.\nY is directly derived from X and so we see a large importance for X in predicting Y. The intercept doesn’t have much meaning in this contrived example, but could be meaningful depending on the dataset.\nThe performance after bootstrapping the patient probabilities has less variance compared to the first graph, but is within the middle 50% of that distribution. This performance measure should be used because it has reduced bias from sampling all computed patient probabilities.\nThe average patient probabilities across bootstraps show the average prediction by that classifier on an individual basis, as well as the per patient variance. This allows for examining further how variable a model can be for certain patient subpopulations for the classification task.\n\n\n\nGo further with MCCV\n\nYou can compute random permutations in order to compare your ‘observed’ model to just randomly shuffling your data. You can make similar plots with data from the method run_permuted_mccv:\n\n\n\nShow The Code\nmccv_obj.run_permuted_mccv()\n\n\n\nSee the model evaluations and validations in these references for some ideas like model calibration and deriving model equations:\n\nGiangreco, N.P., Lebreton, G., Restaino, S. et al. Alterations in the kallikrein-kinin system predict death after heart transplant. Sci Rep 12, 14167 (2022). https://doi.org/10.1038/s41598-022-18573-2\nGiangreco et al. 2021. Plasma kallikrein predicts primary graft dysfunction after heart transplant. Journal of Heart and Lung Transplantation, 40(10), 1199-1211. https://doi.org/10.1016/j.healun.2021.07.001."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Monte Carlo Cross Validation",
    "section": "",
    "text": "Evidentiary and interpretable prediction\nBinary and Multi-classification algorithm for adverse outcome detection, survival classification, and endpoint prediction (see references for details)\n\n\n\nObjectives of this project\n\nBuild the mccv python package: easily implement and perform MCCV for learning and prediction tasks.\nShowcase accessibly to build, validate, and interpret MCCV classifiers.\nDemonstrate use in both python and R for diverse community implementations.\n\n\n\nInstallation\nmkdir ~/my_directory #choose where to clone the mccv repository\ncd ~/my_directory\ngit clone https://github.com/ngiangre/mccv.git\ncd mccv/\npython3 -m pip install .\n\n\nUsage\n\nPythonR\n\n\n\nimport pandas as pd\ndata = pd.read_csv('data/data.csv',index_col=0) # Feature column name is 'biomarker' and response column  name is 'status'\ndata.head()\n\n     status  biomarker\nobs                   \n1         0   1.665731\n2         0  -0.875837\n3         0  -1.391374\n4         0  -0.297352\n5         1   0.189857\n\n\n\nimport mccv\nmccv_obj = mccv.mccv(num_bootstraps=200)\nmccv_obj.set_X( data.loc[:,['biomarker']] )\nmccv_obj.set_Y( data.loc[:,['status']] )\nmccv_obj.run_mccv()\nmccv_obj.run_permuted_mccv()\n\n#Output\nfor n in mccv_obj.mccv_data:\n    print(n)\n    mccv_obj.mccv_data[n].head()\n\nModel Learning\n   bootstrap                model  ...  train_roc_auc  validation_roc_auc\n0          0  Logistic Regression  ...       0.529453            0.611111\n1          1  Logistic Regression  ...       0.515235            0.732143\n2          2  Logistic Regression  ...       0.543056            0.400000\n3          3  Logistic Regression  ...       0.519728            0.727273\n4          4  Logistic Regression  ...       0.554054            0.574074\n\n[5 rows x 5 columns]\nFeature Importance\n   bootstrap    feature  importance                model\n0          0  biomarker    1.009705  Logistic Regression\n1          0  Intercept   -0.598575  Logistic Regression\n0          1  biomarker    0.509433  Logistic Regression\n1          1  Intercept   -0.226550  Logistic Regression\n0          2  biomarker    1.598627  Logistic Regression\nPatient Predictions\n     bootstrap                model  y_pred   y_proba  y_true\nobs                                                          \n27           0  Logistic Regression       0  0.384723       1\n87           0  Logistic Regression       1  0.601359       0\n3            0  Logistic Regression       0  0.401320       0\n56           0  Logistic Regression       1  0.512481       1\n76           0  Logistic Regression       0  0.393009       0\nPerformance\n                 model   metric  performance_bootstrap     value\n0  Logistic Regression  roc_auc                      0  0.467487\n1  Logistic Regression  roc_auc                      1  0.467776\n2  Logistic Regression  roc_auc                      2  0.480176\n3  Logistic Regression  roc_auc                      3  0.480679\n4  Logistic Regression  roc_auc                      4  0.475859\n\nfor n in mccv_obj.mccv_permuted_data:\n    print(n)\n    mccv_obj.mccv_permuted_data[n].head()\n\nModel Learning\n   bootstrap                model  ...  train_roc_auc  validation_roc_auc\n0          0  Logistic Regression  ...       0.506233            0.642857\n1          1  Logistic Regression  ...       0.492030            0.703704\n2          2  Logistic Regression  ...       0.510135            0.537037\n3          3  Logistic Regression  ...       0.506944            0.703704\n4          4  Logistic Regression  ...       0.589547            0.340909\n\n[5 rows x 5 columns]\nFeature Importance\n   bootstrap    feature  importance                model\n0          0  biomarker   -0.196116  Logistic Regression\n1          0  Intercept    0.079220  Logistic Regression\n0          1  biomarker   -0.628093  Logistic Regression\n1          1  Intercept    0.236617  Logistic Regression\n0          2  biomarker    0.166196  Logistic Regression\nPatient Predictions\n     bootstrap                model  y_pred   y_proba  y_true\nobs                                                          \n27           0  Logistic Regression       1  0.513536       1\n87           0  Logistic Regression       0  0.470809       0\n3            0  Logistic Regression       1  0.510160       0\n56           0  Logistic Regression       0  0.488317       1\n76           0  Logistic Regression       1  0.511844       1\nPerformance\n                 model   metric  performance_bootstrap     value\n0  Logistic Regression  roc_auc                      0  0.440616\n1  Logistic Regression  roc_auc                      1  0.442506\n2  Logistic Regression  roc_auc                      2  0.449941\n3  Logistic Regression  roc_auc                      3  0.440162\n4  Logistic Regression  roc_auc                      4  0.449896\n\n\n\n\n\nif(!requireNamespace(\"readr\")){install.packages(\"readr\")}\n\nLoading required namespace: readr\n\nlibrary(readr)\ndata <- read_csv(\"data/data.csv\",col_types = c(\"iid\")) #set obs as integer, status as integer, and biomarker as double\nhead(data)\n\n# A tibble: 6 × 3\n    obs status biomarker\n  <int>  <int>     <dbl>\n1     1      0     1.67 \n2     2      0    -0.876\n3     3      0    -1.39 \n4     4      0    -0.297\n5     5      1     0.190\n6     6      0     2.20 \n\n\n\nif(!requireNamespace(\"reticulate\")){install.packages(\"reticulate\")}\nmccv = reticulate::import('mccv')\nmccv_obj = mccv$mccv(num_bootstraps = as.integer(200))\n\nX = reticulate::r_to_py(data[,c('obs','biomarker')])\nX = X$set_index(reticulate::r_to_py('obs'))\n\ny = reticulate::r_to_py(data[,c('obs','status')])\ny = y$set_index(reticulate::r_to_py('obs'))\n\nmccv_obj$set_X(X)\nmccv_obj$set_Y(y)\nmccv_obj$run_mccv()\nmccv_obj$run_permuted_mccv()\n\n#Output\nlapply(mccv_obj$mccv_data,head)\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\n\n$`Model Learning`\n  bootstrap               model test_roc_auc train_roc_auc validation_roc_auc\n1         0 Logistic Regression       1.0000     0.5294525          0.6111111\n2         1 Logistic Regression       0.8000     0.5152355          0.7321429\n3         2 Logistic Regression       1.0000     0.5430556          0.4000000\n4         3 Logistic Regression       0.8750     0.5197279          0.7272727\n5         4 Logistic Regression       0.8125     0.5540541          0.5740741\n6         5 Logistic Regression       1.0000     0.5499325          0.5357143\n\n$`Feature Importance`\n  bootstrap   feature importance               model\n1         0 biomarker  1.0097049 Logistic Regression\n2         0 Intercept -0.5985751 Logistic Regression\n3         1 biomarker  0.5094328 Logistic Regression\n4         1 Intercept -0.2265503 Logistic Regression\n5         2 biomarker  1.5986271 Logistic Regression\n6         2 Intercept -0.9420031 Logistic Regression\n\n$`Patient Predictions`\n  bootstrap               model y_pred   y_proba y_true\n1         0 Logistic Regression      0 0.3847230      1\n2         0 Logistic Regression      1 0.6013587      0\n3         0 Logistic Regression      0 0.4013202      0\n4         0 Logistic Regression      1 0.5124811      1\n5         0 Logistic Regression      0 0.3930090      0\n6         0 Logistic Regression      0 0.4660667      1\n\n$Performance\n                model  metric performance_bootstrap     value\n1 Logistic Regression roc_auc                     0 0.4674874\n2 Logistic Regression roc_auc                     1 0.4677764\n3 Logistic Regression roc_auc                     2 0.4801763\n4 Logistic Regression roc_auc                     3 0.4806793\n5 Logistic Regression roc_auc                     4 0.4758592\n6 Logistic Regression roc_auc                     5 0.4687351\n\nlapply(mccv_obj$mccv_permuted_data,head)\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\nWarning in py_to_r.pandas.core.frame.DataFrame(object): index contains\nduplicated values: row names not set\n\n\n$`Model Learning`\n  bootstrap               model test_roc_auc train_roc_auc validation_roc_auc\n1         0 Logistic Regression       0.5500     0.5062327          0.6428571\n2         1 Logistic Regression       0.8000     0.4920305          0.7037037\n3         2 Logistic Regression       0.5625     0.5101351          0.5370370\n4         3 Logistic Regression       0.8000     0.5069444          0.7037037\n5         4 Logistic Regression       0.9000     0.5895470          0.3409091\n6         5 Logistic Regression       0.7000     0.5360111          0.5178571\n\n$`Feature Importance`\n  bootstrap   feature  importance               model\n1         0 biomarker -0.19611610 Logistic Regression\n2         0 Intercept  0.07921951 Logistic Regression\n3         1 biomarker -0.62809256 Logistic Regression\n4         1 Intercept  0.23661698 Logistic Regression\n5         2 biomarker  0.16619555 Logistic Regression\n6         2 Intercept -0.01455491 Logistic Regression\n\n$`Patient Predictions`\n  bootstrap               model y_pred   y_proba y_true\n1         0 Logistic Regression      1 0.5135363      1\n2         0 Logistic Regression      0 0.4708091      0\n3         0 Logistic Regression      1 0.5101595      0\n4         0 Logistic Regression      0 0.4883168      1\n5         0 Logistic Regression      1 0.5118443      1\n6         0 Logistic Regression      0 0.4973405      1\n\n$Performance\n                model  metric performance_bootstrap     value\n1 Logistic Regression roc_auc                     0 0.4406164\n2 Logistic Regression roc_auc                     1 0.4425061\n3 Logistic Regression roc_auc                     2 0.4499406\n4 Logistic Regression roc_auc                     3 0.4401616\n5 Logistic Regression roc_auc                     4 0.4498963\n6 Logistic Regression roc_auc                     5 0.4436607\n\n\n\n\n\n\n\nContribute\nPlease do! Reach out to Nick directly (nick.giangreco@gmail.com), make an issue, or make a pull request.\n\n\nLicense\nThis software is released under the MIT license, which can be found in LICENSE in the root directory of this repository.\n\n\nCitation\nGiangreco, N.P., Lebreton, G., Restaino, S. et al. Alterations in the kallikrein-kinin system predict death after heart transplant. Sci Rep 12, 14167 (2022). https://doi.org/10.1038/s41598-022-18573-2\nGiangreco et al. 2021. Plasma kallikrein predicts primary graft dysfunction after heart transplant. Journal of Heart and Lung Transplantation, 40(10), 1199-1211. https://doi.org/10.1016/j.healun.2021.07.001."
  },
  {
    "objectID": "mccv_parameters.html",
    "href": "mccv_parameters.html",
    "title": "Learning Parameters",
    "section": "",
    "text": "Refitting the model per learned parameters from K-fold cross validation during training is a key step in the Monte Carlo Cross Validation (MCCV) methodology. However, the run_mccv() function that runs the entire MCCV procedure does not return these learned parameters. In this article, the learned parameters are extracted from 10-fold cross validation of a Logistic Regression model. These learned parameters are 200 intercepts and coefficients later refit to the entire training set. Here, we visualize the distribution of the learned parameters to illustrate the learning process.\n\n\nShow The Code\nimport numpy as np\nimport scipy as sc\nimport pandas as pd\n\nN=100\nnp.random.seed(0)\nZ1 = np.random.beta(2,3,size=N,)\nnp.random.seed(0)\nZ2 = np.random.beta(2,2.5,size=N)\nZ = np.concatenate([Z1,Z2])\n\nY = np.concatenate([np.repeat(0,N),np.repeat(1,N)])\ndf = pd.DataFrame(data={'Y' : Y,'Z' : Z})\ndf.index.name = 'pt'\n\n\n\n\nShow The Code\nlibrary(tidyverse)\ndf <- tibble::tibble(\n    Y = reticulate::py$Y,\n    Z = reticulate::py$Z\n)\ndf[['Y']] <- factor(df$Y,levels=c(0,1))\n\ndf %>% \n    ggplot(aes(Y,Z)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    labs(x = \"Response\",y=\"Predictor\") +\n    theme_bw(base_size = 16)\n\n\n\n\n\n\n\nShow The Code\nimport mccv\nimport pandas as pd\n\nmccv_obj = mccv.mccv(num_bootstraps=200,n_jobs=4)    \nmccv_obj.set_X(df[['Z']])\nmccv_obj.set_Y(df[['Y']])\n\nparam_dfs=[]\nfor seed in range(200):\n    retrained_fit = mccv_obj.mccv(seed)[1]['Logistic Regression'].__dict__\n    param_dict = {k : retrained_fit[k].reshape(-1)[0] for k in ('coef_','intercept_')}\n    param_df = pd.DataFrame.from_dict(param_dict,orient='index',dtype='object',columns=[seed])\n    param_dfs.append(param_df)\nretrained_parameters_df = pd.concat(param_dfs,axis=1).T\nretrained_parameters_df.index.name='seed'\nretrained_parameters_df.reset_index(inplace=True)\n\n\n\n\nShow The Code\nlibrary(tidyverse)\n\nplot_dat <- \n    reticulate::py$retrained_parameters_df %>% \n    pivot_longer(\n        cols = contains('_')\n    ) %>% \n    mutate(\n        seed = as.integer(seed),\n        name = factor(name,levels=c(\"intercept_\",\"coef_\"),labels=c(\"Intercept\",\"Coefficient\")),\n        value = as.double(value)\n        )\n\np <- \n    plot_dat %>% \n    ggplot(aes(name,value,group=seed)) +\n    geom_line() +\n    scale_x_discrete(\n        expand = expansion(0.01,0.01),\n        name=NULL\n    ) +\n    scale_y_continuous(\n        name=\"Parameter Values\"\n    ) +\n    labs(caption='Estimated Parameters From 200 Logistic Regression Bootstraps') +\n    theme_bw(base_size = 20) +\n    theme(\n        axis.text.x = element_text(angle=45,hjust=1,vjust=1)\n    )\n\np + \n    stat_summary(fun=mean,color='red',geom='line',aes(group=1),linewidth=3) +\n    labs(caption='Estimated Parameters From 200 Logistic Regression Bootstraps\\nAverage in Red')\n\n\n\n\n\nShow The Code\navg_values <- \n    summarise(plot_dat,`Average Value` = mean(value),.by=name) %>% \n    mutate(across(where(is.numeric),function(x)round(x,2)))\n\navg_values %>% \n    gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      name\n      Average Value\n    \n  \n  \n    Coefficient\n0.94\n    Intercept\n-0.44\n  \n  \n  \n\n\n\n\nWe learned that on average when the predictor is 0, there is a -0.44 log of the odds or 0.64 odds or 0.39 probability to be in the outcome group (Y=1).\nOn average, there is a 0.94 expected change in the log of the odds or 2.56 odds for a one-unit increase in the predictor. In other words, we expect to see a 72% increase in the odds of being in the outcome group (Y=1) as the predictor increases by one-unit.\n\n\nShow The Code\nlibrary(gganimate)\n#install.packages(\"transformr\")\nanimp <- \n    p + \n    transition_time(seed) + \n    labs(title=\"Seed : {frame_time}\")\nanimate(animp,duration = 5, fps = 20, width = 500, height = 500, renderer = gifski_renderer())\n\n\n\n\n\nShow The Code\nanim_save(\"mccv_parameters_animation.gif\")\n\n\nOn the other hand, MCCV is unable to learn stable parameter values when the data is shuffled:\n\n\nShow The Code\nimport mccv\nimport pandas as pd\n\nmccv_obj = mccv.mccv(num_bootstraps=200,n_jobs=4)    \nmccv_obj.set_X(df[['Z']])\nmccv_obj.set_Y(df[['Y']])\n\nparam_dfs=[]\nfor seed in range(200):\n    retrained_fit = mccv_obj.permuted_mccv(seed)[1]['Logistic Regression'].__dict__\n    param_dict = {k : retrained_fit[k].reshape(-1)[0] for k in ('coef_','intercept_')}\n    param_df = pd.DataFrame.from_dict(param_dict,orient='index',dtype='object',columns=[seed])\n    param_dfs.append(param_df)\nretrained_parameters_df = pd.concat(param_dfs,axis=1).T\nretrained_parameters_df.index.name='seed'\nretrained_parameters_df.reset_index(inplace=True)\n\n\n\n\nShow The Code\nlibrary(tidyverse)\n\nplot_dat <- \n    reticulate::py$retrained_parameters_df %>% \n    pivot_longer(\n        cols = contains('_')\n    ) %>% \n    mutate(\n        seed = as.integer(seed),\n        name = factor(name,levels=c(\"intercept_\",\"coef_\"),labels=c(\"Intercept\",\"Coefficient\")),\n        value = as.double(value)\n        )\n\np <- \n    plot_dat %>% \n    ggplot(aes(name,value,group=seed)) +\n    geom_line() +\n    scale_x_discrete(\n        expand = expansion(0.01,0.01),\n        name=NULL\n    ) +\n    scale_y_continuous(\n        name=\"Parameter Values\"\n    ) +\n    labs(caption='Permuted MCCV\\nEstimated Parameters From 200 Logistic Regression Bootstraps') +\n    theme_bw(base_size = 20) +\n    theme(\n        axis.text.x = element_text(angle=45,hjust=1,vjust=1)\n    )\n\np + \n    stat_summary(fun=mean,color='red',geom='line',aes(group=1),linewidth=3) +\n    labs(caption='Permuted MCCV\\nEstimated Parameters From 200 Logistic Regression Bootstraps\\nAverage in Red')\n\n\n\n\n\nShow The Code\navg_values <- \n    summarise(plot_dat,`Average Value` = mean(value),.by=name) %>% \n    mutate(across(where(is.numeric),function(x)round(x,2)))\n\navg_values %>% \n    gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      name\n      Average Value\n    \n  \n  \n    Coefficient\n-0.02\n    Intercept\n0.01\n  \n  \n  \n\n\n\n\n\n\nShow The Code\nlibrary(gganimate)\n#install.packages(\"transformr\")\nanimp <- \n    p + \n    transition_time(seed) + \n    labs(title=\"Seed : {frame_time}\")\nanimate(animp,duration = 5, fps = 20, width = 500, height = 500, renderer = gifski_renderer())\n\n\n\n\n\nShow The Code\nanim_save(\"permuted_mccv_parameters_animation.gif\")\n\n\nThe distribution of parameters from the permuted MCCV provides a null distribution. The alternative hypothesis is learned parameters from MCCV (using the real data) is different from parameters estimated from permuted MCCV (using shuffled data)."
  },
  {
    "objectID": "mccv_power.html",
    "href": "mccv_power.html",
    "title": "Estimating Power",
    "section": "",
    "text": "One of the advantages of using MCCV is it’s data-driven approach. Given data, you can answer questions such as:\n\nHow predictive is Y from X?\nIs a certain cut of the data driving the prediction?\nIs there another variable obscuring or influencing the contribution of X in predicting Y?\n\nIn this article I want to illustrate another type of question MCCV can address:\n\nHow does the prediction of Y by X compare to the prediction by X to a random Y? In other words, what is the power of my data X to predict Y?\n\nI will discuss at the end how the term ‘power’ is used here in comparison to the statistical and more common form of the term. First, I will show two examples showing the power of X to predict Y.\nThis first example shows low power\n\n\nShow The Code\nimport numpy as np\nN=100\nX1 = np.random.normal(loc=0,scale=1,size=N)\nX2 = np.random.normal(loc=0.5,scale=1,size=N)\nX = np.concatenate([X1,X2])\nY = np.concatenate([np.repeat(0,N),np.repeat(1,N)])\n\nimport pandas as pd\ndf = pd.DataFrame(data={'Y' : Y,'X' : X})\ndf.index.name = 'pt'\n\n\n\n\nShow The Code\nlibrary(tidyverse)\n\ndf <- reticulate::py$df\n\ndf[['Y']] <- factor(df$Y,levels=c(0,1))\n\ndf %>% \n    ggplot(aes(Y,X)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    labs(x = \"Response\",y=\"Predictor\",title = \"Predictor values for Class One are 50% larger on average\") +\n    theme_bw(base_size = 16)\n\n\n\n\n\n\n\nShow The Code\nimport mccv\n\nmccv_obj = mccv.mccv(num_bootstraps=200,n_jobs=4)\nmccv_obj.set_X(df[['X']])\nmccv_obj.set_Y(df[['Y']])\nmccv_obj.run_mccv()\nmccv_obj.mccv_data['Feature Importance'].insert(\n    len(mccv_obj.mccv_data['Feature Importance'].columns),\n    'type',\n    'real'\n)\nmccv_obj.mccv_data['Model Learning'].insert(\n    len(mccv_obj.mccv_data['Model Learning'].columns),\n    'type',\n    'real'\n)\nmccv_obj.mccv_data['Patient Predictions'].insert(\n    len(mccv_obj.mccv_data['Patient Predictions'].columns),\n    'type',\n    'real'\n)\nmccv_obj.run_permuted_mccv()\nmccv_obj.mccv_permuted_data['Feature Importance'].insert(\n    len(mccv_obj.mccv_permuted_data['Feature Importance'].columns),\n    'type',\n    'permuted'\n)\nmccv_obj.mccv_permuted_data['Model Learning'].insert(\n    len(mccv_obj.mccv_permuted_data['Model Learning'].columns),\n    'type',\n    'permuted'\n)\nmccv_obj.mccv_permuted_data['Patient Predictions'].insert(\n    len(mccv_obj.mccv_permuted_data['Patient Predictions'].columns),\n    'type',\n    'permuted'\n)\n\nfimp_df = \\\npd.concat([\n    (mccv_obj.mccv_data['Feature Importance'].\n    query('feature==\"X\"').\n    reset_index(drop=True)),\n    (mccv_obj.mccv_permuted_data['Feature Importance'].\n    query('feature==\"X\"').\n    reset_index(drop=True))\n])\nppred_df = \\\npd.concat([\n    (mccv_obj.mccv_data['Patient Predictions'].\n    drop('y_pred',axis=1).\n    reset_index()),\n    (mccv_obj.mccv_permuted_data['Patient Predictions'].\n    reset_index().\n    drop('y_pred',axis=1))\n])\npred_df = \\\npd.concat([\n    (mccv_obj.mccv_data['Model Learning'].\n    reset_index()),\n    (mccv_obj.mccv_permuted_data['Model Learning'].\n    reset_index())\n])\n\n\n\n\nShow The Code\npks <- \n    ks.test(\n    reticulate::py$pred_df %>% \n        filter(type=='real') %>% \n        pull(validation_roc_auc),\n    reticulate::py$pred_df %>% \n        filter(type=='permuted') %>% \n        pull(validation_roc_auc),\n    alternative = 'greater'\n)[['p.value']]\n\npwilcox <- \n    wilcox.test(\n    reticulate::py$pred_df %>% \n        filter(type=='real') %>% \n        pull(validation_roc_auc),\n    reticulate::py$pred_df %>% \n        filter(type=='permuted') %>% \n        pull(validation_roc_auc)\n)[['p.value']]\n\nreticulate::py$pred_df %>% \n    ggplot(aes(validation_roc_auc,fill=type)) +\n    geom_histogram(aes(y=after_stat(count)),alpha=.5,binwidth = .05) +\n    scale_fill_brewer(palette = 'Set1',direction = -1,\n                      guide=guide_legend(title=NULL)) +\n    theme_bw() +\n    scale_x_continuous(\n        breaks=seq(0,1,0.05),\n    ) +\n    labs(x='Validation AUROC from Model Learning',\n         y='Number of Validation AUROC values',\n         caption=paste0('Wilcox p-value = ',scales::scientific(pwilcox,3),'\\n',\n                        'Kolmogorov-Smirnov p-value = ',scales::scientific(pks,3),'\\n',\n                        'Real validation AUROC values are greater than Permuted validation AUROC values'))\n\n\n\n\n\n\n\nShow The Code\npks <- \n    ks.test(\n    reticulate::py$fimp_df %>% \n        filter(type=='real') %>% \n        pull(importance),\n    reticulate::py$fimp_df %>% \n        filter(type=='permuted') %>% \n        pull(importance)\n)[['p.value']]\n\npwilcox <- \n    wilcox.test(\n    reticulate::py$fimp_df %>% \n        filter(type=='real') %>% \n        pull(importance),\n    reticulate::py$fimp_df %>% \n        filter(type=='permuted') %>% \n        pull(importance)\n)[['p.value']]\n\nreticulate::py$fimp_df %>% \n    ggplot(aes(importance,fill=type)) +\n    geom_histogram(aes(y=after_stat(count)),alpha=.5,binwidth = .2) +\n    scale_fill_brewer(palette = 'Set1',direction = -1,\n                      guide=guide_legend(title=NULL)) +\n    theme_bw() +\n    labs(x='Importance of X in Predicting Y',\n         y='Number of X importance values',\n         caption=paste0('Wilcox p-value = ',scales::scientific(pwilcox,3),'\\n',\n                        'Kolmogorov-Smirnov p-value = ',scales::scientific(pks,3),'\\n',\n                        'Real X importance values are greater than Permuted X importance values'))\n\n\n\n\n\n\n\nShow The Code\npks <- \n    ks.test(\n    reticulate::py$ppred_df %>% \n    filter(type=='real' & y_true==1) %>% \n    pull(y_proba),\n    reticulate::py$ppred_df %>% \n    filter(type=='permuted' & y_true==1) %>% \n    pull(y_proba)\n)[['p.value']]\npwilcox <- \n    wilcox.test(\n    reticulate::py$ppred_df %>% \n    filter(type=='real' & y_true==1) %>% \n    pull(y_proba),\n    reticulate::py$ppred_df %>% \n    filter(type=='permuted' & y_true==1) %>% \n    pull(y_proba)\n)[['p.value']]\n\nreticulate::py$ppred_df %>% \n    arrange(pt,bootstrap) %>%\n    ggplot(aes(y_proba,fill=factor(y_true),group=y_true)) +\n    geom_histogram(aes(y=after_stat(count)),alpha=.5,binwidth = .01) +\n    scale_fill_brewer(NULL,\n                      palette = 'Set1',\n                      direction = -1,\n                      labels=c(\"Class 0\",\"Class 1\"),\n                      guide=guide_legend(title=NULL)) +\n    facet_wrap(~type,ncol=1,scales='free') +\n    theme_bw() +\n    labs(x='Prediction Probability',\n         y='Number of Prediction Probabilities',\n         caption = paste0('Wilcox p-value = ',scales::scientific(pwilcox,3),'\\n',\n                          'Kolmogorov-Smirnov p-value = ',scales::scientific(pks,3),'\\n',\n                        'Real Class 1 probabilities are greater than Permuted Class 1 probabilities'))\n\n\n\n\n\nThis next example shows high power\n\n\nShow The Code\nimport numpy as np\nN=100\nX1 = np.random.normal(loc=0,scale=1,size=N)\nX2 = np.random.normal(loc=2,scale=1,size=N)\nX = np.concatenate([X1,X2])\nY = np.concatenate([np.repeat(0,N),np.repeat(1,N)])\n\nimport pandas as pd\ndf = pd.DataFrame(data={'Y' : Y,'X' : X})\ndf.index.name = 'pt'\n\n\n\n\nShow The Code\nlibrary(tidyverse)\n\ndf <- reticulate::py$df\n\ndf[['Y']] <- factor(df$Y,levels=c(0,1))\n\ndf %>% \n    ggplot(aes(Y,X)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    labs(x = \"Response\",y=\"Predictor\",title = \"Predictor values for Class One are 200% larger on average\") +\n    theme_bw(base_size = 16)\n\n\n\n\n\n\n\nShow The Code\nimport mccv\n\nmccv_obj = mccv.mccv(num_bootstraps=200,n_jobs=4)\nmccv_obj.set_X(df[['X']])\nmccv_obj.set_Y(df[['Y']])\nmccv_obj.run_mccv()\nmccv_obj.mccv_data['Feature Importance'].insert(\n    len(mccv_obj.mccv_data['Feature Importance'].columns),\n    'type',\n    'real'\n)\nmccv_obj.mccv_data['Model Learning'].insert(\n    len(mccv_obj.mccv_data['Model Learning'].columns),\n    'type',\n    'real'\n)\nmccv_obj.mccv_data['Patient Predictions'].insert(\n    len(mccv_obj.mccv_data['Patient Predictions'].columns),\n    'type',\n    'real'\n)\nmccv_obj.run_permuted_mccv()\nmccv_obj.mccv_permuted_data['Feature Importance'].insert(\n    len(mccv_obj.mccv_permuted_data['Feature Importance'].columns),\n    'type',\n    'permuted'\n)\nmccv_obj.mccv_permuted_data['Model Learning'].insert(\n    len(mccv_obj.mccv_permuted_data['Model Learning'].columns),\n    'type',\n    'permuted'\n)\nmccv_obj.mccv_permuted_data['Patient Predictions'].insert(\n    len(mccv_obj.mccv_permuted_data['Patient Predictions'].columns),\n    'type',\n    'permuted'\n)\n\nfimp_df = \\\npd.concat([\n    (mccv_obj.mccv_data['Feature Importance'].\n    query('feature==\"X\"').\n    reset_index(drop=True)),\n    (mccv_obj.mccv_permuted_data['Feature Importance'].\n    query('feature==\"X\"').\n    reset_index(drop=True))\n])\nppred_df = \\\npd.concat([\n    (mccv_obj.mccv_data['Patient Predictions'].\n    drop('y_pred',axis=1).\n    reset_index()),\n    (mccv_obj.mccv_permuted_data['Patient Predictions'].\n    reset_index().\n    drop('y_pred',axis=1))\n])\npred_df = \\\npd.concat([\n    (mccv_obj.mccv_data['Model Learning'].\n    reset_index()),\n    (mccv_obj.mccv_permuted_data['Model Learning'].\n    reset_index())\n])\n\n\n\n\nShow The Code\npks <- \n    ks.test(\n    reticulate::py$pred_df %>% \n        filter(type=='real') %>% \n        pull(validation_roc_auc),\n    reticulate::py$pred_df %>% \n        filter(type=='permuted') %>% \n        pull(validation_roc_auc),\n    alternative = 'greater'\n)[['p.value']]\n\npwilcox <- \n    wilcox.test(\n    reticulate::py$pred_df %>% \n        filter(type=='real') %>% \n        pull(validation_roc_auc),\n    reticulate::py$pred_df %>% \n        filter(type=='permuted') %>% \n        pull(validation_roc_auc)\n)[['p.value']]\n\nreticulate::py$pred_df %>% \n    ggplot(aes(validation_roc_auc,fill=type)) +\n    geom_histogram(aes(y=after_stat(count)),alpha=.5,binwidth = .05) +\n    scale_fill_brewer(palette = 'Set1',direction = -1,\n                      guide=guide_legend(title=NULL)) +\n    theme_bw() +\n    scale_x_continuous(\n        breaks=seq(0,1,0.05),\n    ) +\n    labs(x='Validation AUROC from Model Learning',\n         y='Number of Validation AUROC values',\n         caption=paste0('Wilcox p-value = ',scales::scientific(pwilcox,3),'\\n',\n                        'Kolmogorov-Smirnov p-value = ',scales::scientific(pks,3),'\\n',\n                        'Real validation AUROC values are greater than Permuted validation AUROC values'))\n\n\n\n\n\n\n\nShow The Code\npks <- \n    ks.test(\n    reticulate::py$fimp_df %>% \n        filter(type=='real') %>% \n        pull(importance),\n    reticulate::py$fimp_df %>% \n        filter(type=='permuted') %>% \n        pull(importance)\n)[['p.value']]\n\npwilcox <- \n    wilcox.test(\n    reticulate::py$fimp_df %>% \n        filter(type=='real') %>% \n        pull(importance),\n    reticulate::py$fimp_df %>% \n        filter(type=='permuted') %>% \n        pull(importance)\n)[['p.value']]\n\nreticulate::py$fimp_df %>% \n    ggplot(aes(importance,fill=type)) +\n    geom_histogram(aes(y=after_stat(count)),alpha=.5,binwidth = .2) +\n    scale_fill_brewer(palette = 'Set1',direction = -1,\n                      guide=guide_legend(title=NULL)) +\n    theme_bw() +\n    labs(x='Importance of X in Predicting Y',\n         y='Number of X importance values',\n         caption=paste0('Wilcox p-value = ',scales::scientific(pwilcox,3),'\\n',\n                        'Kolmogorov-Smirnov p-value = ',scales::scientific(pks,3),'\\n',\n                        'Real X importance values are greater than Permuted X importance values'))\n\n\n\n\n\n\n\nShow The Code\npks <- \n    ks.test(\n    reticulate::py$ppred_df %>% \n    filter(type=='real' & y_true==1) %>% \n    pull(y_proba),\n    reticulate::py$ppred_df %>% \n    filter(type=='permuted' & y_true==1) %>% \n    pull(y_proba)\n)[['p.value']]\npwilcox <- \n    wilcox.test(\n    reticulate::py$ppred_df %>% \n    filter(type=='real' & y_true==1) %>% \n    pull(y_proba),\n    reticulate::py$ppred_df %>% \n    filter(type=='permuted' & y_true==1) %>% \n    pull(y_proba)\n)[['p.value']]\n\nreticulate::py$ppred_df %>% \n    arrange(pt,bootstrap) %>%\n    ggplot(aes(y_proba,fill=factor(y_true),group=y_true)) +\n    geom_histogram(aes(y=after_stat(count)),alpha=.5,binwidth = .01) +\n    scale_fill_brewer(NULL,\n                      palette = 'Set1',\n                      direction = -1,\n                      labels=c(\"Class 0\",\"Class 1\"),\n                      guide=guide_legend(title=NULL)) +\n    facet_wrap(~type,ncol=1,scales='free') +\n    theme_bw() +\n    labs(x='Prediction Probability',\n         y='Number of Prediction Probabilities',\n         caption = paste0('Wilcox p-value = ',scales::scientific(pwilcox,3),'\\n',\n                          'Kolmogorov-Smirnov p-value = ',scales::scientific(pks,3),'\\n',\n                        'Real Class 1 probabilities are greater than Permuted Class 1 probabilities'))\n\n\n\n\n\nThese two examples show different metrics for defining a ‘powerful’ prediction. Here, a powerful prediction can only be defined by the combination of different components to describe how and to what degree a predictor X can predict a response Y. These components can come from the model learning process as well as the applying the model on new data i.e. a validation set. Defining a powerful prediction is difficult using only one quantitative metric, like the statistical tests shown in the plot captions. But a few metrics noted here (and probably others I’ve mistakingly overlooked) can accurately define a powerful prediction:\n\nThe average AUROC for the validation set should be more than 50%. To be more strict, the 95% confidence interval should be greater than 50% AUROC.\nThe importance value (beta coefficient for logistic regression) of X for predicting Y should be above the null association i.e. 0 and should barely overlap the importance values for a shuffled response. Statistical tests will probably produce a false positive by showing a small p-value, like here, so they shouldn’t be the metric in defining a powerful prediction.\nThere seems to be at least two takeaways from the distributions of the prediction probabilities. First, the distribution of the real predicted probabilities for class 1 need to be greater than that for class 0. Also the distribution of real, class 1 predicted probabilities need to be greater than the distribution of permuted, class 1 predicted probabilities.\n\nI used a combination of the metrics referenced above in the papers published using MCCV. This article’s toy examples illustrate why these metrics are sensible for defining the power of a prediction given the data."
  },
  {
    "objectID": "mccv_robust.html",
    "href": "mccv_robust.html",
    "title": "Estimating Robustness",
    "section": "",
    "text": "Often, a question of interest is at what sample size (used in training a model) can I detect an effect? Furthermore, is the effect I’m detecting a robust signal? MCCV can estimate the robustness of an effect or signal by learning from data at different sizes of a sample. For example, a large effect or performance, say 0.8 AUROC, may be reached using all data available in the sample. However, can I still reach 0.8 AUROC at a smaller sample size? Also, is the detected signal robust to the size of the sample used in training the model or does a particular cut of the data drive model learning? This article will show how learning from varying sample sizes may or may not show robust signal aka detection of an effect representative of the data generating process.\nThis first example defines two classes of data (class 1 and class 2) with a predictor drawn from very similar distributions. I expect robust signal (i.e. AUROC) to be detected as the proportion of samples increases:\n\n\nShow The Code\nimport numpy as np\nN=100\nnp.random.seed(0)\nZ1 = np.random.beta(2,3,size=N,)\nnp.random.seed(0)\nZ2 = np.random.beta(2,2.5,size=N)\nZ = np.concatenate([Z1,Z2])\n\nimport scipy as sc\nY = np.concatenate([np.repeat(0,N),np.repeat(1,N)])\n\nimport pandas as pd\ndf = pd.DataFrame(data={'Y' : Y,'Z' : Z})\ndf.index.name = 'pt'\n\n\n\n\nShow The Code\nlibrary(tidyverse)\n\ndf <- tibble::tibble(\n    Y = reticulate::py$Y,\n    Z = reticulate::py$Z\n)\ndf[['Y']] <- factor(df$Y,levels=c(0,1))\n\ndf %>% \n    ggplot(aes(Y,Z)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    labs(x = \"Response\",y=\"Predictor\") +\n    theme_bw(base_size = 16)\n\n\n\n\n\n\n\nShow The Code\nimport mccv\n\nperf_dfs = []\nfor ts in [.1,.2,.3,.4,.5,.6,.7,.8]:\n    mccv_obj = mccv.mccv(num_bootstraps=200,n_jobs=4)\n    mccv_obj.test_size = ts\n    mccv_obj.set_X(df[['Z']])\n    mccv_obj.set_Y(df[['Y']])\n    mccv_obj.run_mccv()\n    perf_df = mccv_obj.mccv_data['Performance']\n    perf_df.insert(len(perf_df.columns),'training_size',1-ts)\n    perf_df.insert(len(perf_df.columns),'test_size',ts)\n    perf_dfs.append(perf_df)\n\n\n\n\nShow The Code\nreticulate::py$perf_dfs %>% \n    bind_rows() %>% \n    ggplot(aes(factor(training_size),value)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    scale_x_discrete(\n        labels = function(x)paste0(as.double(x)*100,\"%\")\n    ) +\n    labs(x = \"Sample Size for MCCV Training\",y=\"AUROC\",caption=paste0(\n        \"As we increase our sample size for learning,\\n\",\n        \"performance increases as expected,\\n\",\n        \"but so does AUROC variability\")) +\n    theme_bw(base_size = 16)\n\n\n\n\n\nThe second example, instead, defines two classes of data drawn from two different distributions. I would expect non-robust signal detected as the sample size for training is increased.\n\n\nShow The Code\nimport numpy as np\nN=100\nnp.random.seed(0)\nZ1 = np.random.beta(2,2.5,size=N)\nnp.random.seed(0)\nZ2 = np.random.beta(6,5,size=N)\nZ = np.concatenate([Z1,Z2])\n\nimport scipy as sc\nY = np.concatenate([np.repeat(0,N),np.repeat(1,N)])\n\nimport pandas as pd\ndf = pd.DataFrame(data={'Y' : Y,'Z' : Z})\ndf.index.name = 'pt'\n\n\n\n\nShow The Code\nlibrary(tidyverse)\n\ndf <- tibble::tibble(\n    Y = reticulate::py$Y,\n    Z = reticulate::py$Z\n)\ndf[['Y']] <- factor(df$Y,levels=c(0,1))\n\ndf %>% \n    ggplot(aes(Y,Z)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    labs(x = \"Response\",y=\"Predictor\") +\n    theme_bw(base_size = 16)\n\n\n\n\n\n\n\nShow The Code\nimport mccv\n\nperf_dfs = []\nfor ts in [.1,.2,.3,.4,.5,.6,.7,.8]:\n    mccv_obj = mccv.mccv(num_bootstraps=200,n_jobs=4)\n    mccv_obj.test_size = ts\n    mccv_obj.set_X(df[['Z']])\n    mccv_obj.set_Y(df[['Y']])\n    mccv_obj.run_mccv()\n    perf_df = mccv_obj.mccv_data['Performance']\n    perf_df.insert(len(perf_df.columns),'training_size',1-ts)\n    perf_df.insert(len(perf_df.columns),'test_size',ts)\n    perf_dfs.append(perf_df)\n\n\n\n\nShow The Code\nreticulate::py$perf_dfs %>% \n    bind_rows() %>% \n    ggplot(aes(factor(training_size),value)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    scale_x_discrete(\n        labels = function(x)paste0(as.double(x)*100,\"%\")\n    ) +\n    labs(x = \"Sample Size for MCCV Training\",y=\"AUROC\",caption=paste0(\n        \"As we increase our sample size for learning,\\n\",\n        \"performance increases as expected but also stagnates\")) +\n    theme_bw(base_size = 16)\n\n\n\n\n\nIn short, my thinking is the data generating process is captured in a sample only if a robust signal is found. A robust signal can be represented by a linear, average increase in AUROC performance as sample size using for training increases. Otherwise, the signal-to-noise ratio is lower than what would be needed to make generalizable predictions from the specified model and data. In this last example, the evidence is unclear as expected whether the two classes of data are generated by the same process. I say this for two reasons:\n\nThere is stagnant performance between using 30% and 80% of the sample size for training\nThere is a stark difference between using 20% and 90% of the sample size for training. I would expect there to be more overlap compared to complete non-overlap."
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Motivation\n\nEstimate predictions for small data\nEvaluate variable or feature importance on all subpopulations of the data\nGenerate prediction uncertainty and variance\nDevelop classifiers based on unseen data\n\n\n\nMain advantages\n\nPredictions are generated on many different training/validation data splits\nPredictor or feature importances to the dependent variable are generalized over many subpopulations of the data\nNo data leakage - Predictions are on observations not included during training\n\n\n\nProcedural overview\n\nMonte Carlo simulation splits the data into training and validation sets\nK-fold cross validation (10 by default) on the training set is used to estimate “good” model parameters\nThe model with the “good” parameters is fit on the entire training set\nThe refitted model predicts the yet-to-be-seen validation set\nPerformance metrics are generated using resamples (bootstrap with replacement) of the observation probabilities\n\n\n\nRelated Methods\n\nMonte-Carlo simulation (similar to ShuffleSplit in sklearn)\nCross-Validation (tidymodels in R and sklearn in python)\nmc_cv from tidymodels in R, but does not include additional cross validation as explained above\n\n\n\nReferences\n\nK-fold and Montecarlo cross-validation vs Bootstrap: a primer\nCross-Validation: K Fold vs Monte Carlo"
  }
]