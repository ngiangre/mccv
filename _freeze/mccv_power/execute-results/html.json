{
  "hash": "16892c61dabd349dce6c10a128731403",
  "result": {
    "markdown": "---\ntitle: \"Estimating Power Using MCCV\"\nformat:\n  html:\n    code-fold: true\n    code-summary: 'Show The Code'\n---\n\n\nOften, a question of interest is at what sample size (used in training a model) can I detect an effect? Furthermore, is the effect I'm detecting a robust signal? MCCV can estimate the power to detect an effect or robust signal by learning from data at different sizes of a sample. For example, a large effect or performance, say 0.8 AUROC, may be reached using all data available in the sample. However, can I still reach 0.8 AUROC at a smaller sample size? Also, is the detected signal robust to the size of the sample used in training the model? This article will show how learning from varying sample sizes may or may not show robust signal aka detection of an effect representative of the data generating process.\n\nThis first example defines two classes of data (class 1 and class 2) with a predictor drawn from very similar distributions. I expect robust signal (i.e. AUROC) to be detected as the proportion of samples increases:\n\n\n::: {.cell hash='mccv_power_cache/html/unnamed-chunk-1_0f58c2972ae3fd8a604471cdccebff7f'}\n\n```{.python .cell-code}\nimport numpy as np\nN=100\nnp.random.seed(0)\nZ1 = np.random.beta(2,3,size=N,)\nnp.random.seed(0)\nZ2 = np.random.beta(2,2.5,size=N)\nZ = np.concatenate([Z1,Z2])\n\nimport scipy as sc\nY = np.concatenate([np.repeat(0,N),np.repeat(1,N)])\n\nimport pandas as pd\ndf = pd.DataFrame(data={'Y' : Y,'Z' : Z})\ndf.index.name = 'pt'\n```\n:::\n\n::: {.cell hash='mccv_power_cache/html/unnamed-chunk-2_9db30d81c6f05f07d3449c891ff84da5'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndf <- tibble::tibble(\n    Y = reticulate::py$Y,\n    Z = reticulate::py$Z\n)\ndf[['Y']] <- factor(df$Y,levels=c(0,1))\n\ndf %>% \n    ggplot(aes(Y,Z)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    labs(x = \"Response\",y=\"Predictor\") +\n    theme_bw(base_size = 16)\n```\n\n::: {.cell-output-display}\n![](mccv_power_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='mccv_power_cache/html/unnamed-chunk-3_9f4e201b3fdc6e19d0cfdc2ba3a73b88'}\n\n```{.python .cell-code}\nimport mccv\n\nperf_dfs = []\nfor ts in [.1,.2,.3,.4,.5,.6,.7,.8]:\n    mccv_obj = mccv.mccv(num_bootstraps=200,n_jobs=4)\n    mccv_obj.test_size = ts\n    mccv_obj.set_X(df[['Z']])\n    mccv_obj.set_Y(df[['Y']])\n    mccv_obj.run_mccv()\n    perf_df = mccv_obj.mccv_data['Performance']\n    perf_df.insert(len(perf_df.columns),'training_size',1-ts)\n    perf_df.insert(len(perf_df.columns),'test_size',ts)\n    perf_dfs.append(perf_df)\n```\n:::\n\n::: {.cell hash='mccv_power_cache/html/unnamed-chunk-4_3c612063f27a717152d60ebdb22ec86d'}\n\n```{.r .cell-code}\nreticulate::py$perf_dfs %>% \n    bind_rows() %>% \n    ggplot(aes(factor(training_size),value)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    scale_x_discrete(\n        labels = function(x)paste0(as.double(x)*100,\"%\")\n    ) +\n    labs(x = \"Sample Size for MCCV Training\",y=\"AUROC\",caption=paste0(\n        \"As we increase our sample size for learning,\\n\",\n        \"performance increases as expected,\\n\",\n        \"but so does AUROC variability\")) +\n    theme_bw(base_size = 16)\n```\n\n::: {.cell-output-display}\n![](mccv_power_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe second example, instead, defines two classes of data drawn from two different distributions. I would expect non-robust signal detected as the sample size for training is increased.\n\n\n::: {.cell hash='mccv_power_cache/html/unnamed-chunk-5_f3c52c873424b1d4ca99f6a9c144efd8'}\n\n```{.python .cell-code}\nimport numpy as np\nN=100\nnp.random.seed(0)\nZ1 = np.random.beta(2,2.5,size=N)\nnp.random.seed(0)\nZ2 = np.random.beta(6,5,size=N)\nZ = np.concatenate([Z1,Z2])\n\nimport scipy as sc\nY = np.concatenate([np.repeat(0,N),np.repeat(1,N)])\n\nimport pandas as pd\ndf = pd.DataFrame(data={'Y' : Y,'Z' : Z})\ndf.index.name = 'pt'\n```\n:::\n\n::: {.cell hash='mccv_power_cache/html/unnamed-chunk-6_0da1af78aa269dd30dca1e731d1b7eeb'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndf <- tibble::tibble(\n    Y = reticulate::py$Y,\n    Z = reticulate::py$Z\n)\ndf[['Y']] <- factor(df$Y,levels=c(0,1))\n\ndf %>% \n    ggplot(aes(Y,Z)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    labs(x = \"Response\",y=\"Predictor\") +\n    theme_bw(base_size = 16)\n```\n\n::: {.cell-output-display}\n![](mccv_power_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='mccv_power_cache/html/unnamed-chunk-7_3cc8abd734fd8ad212ea2bc6114fcb25'}\n\n```{.python .cell-code}\nimport mccv\n\nperf_dfs = []\nfor ts in [.1,.2,.3,.4,.5,.6,.7,.8]:\n    mccv_obj = mccv.mccv(num_bootstraps=200,n_jobs=4)\n    mccv_obj.test_size = ts\n    mccv_obj.set_X(df[['Z']])\n    mccv_obj.set_Y(df[['Y']])\n    mccv_obj.run_mccv()\n    perf_df = mccv_obj.mccv_data['Performance']\n    perf_df.insert(len(perf_df.columns),'training_size',1-ts)\n    perf_df.insert(len(perf_df.columns),'test_size',ts)\n    perf_dfs.append(perf_df)\n```\n:::\n\n::: {.cell hash='mccv_power_cache/html/unnamed-chunk-8_08627e700ccb55ce4f96a9cbf77771fb'}\n\n```{.r .cell-code}\nreticulate::py$perf_dfs %>% \n    bind_rows() %>% \n    ggplot(aes(factor(training_size),value)) +\n    geom_boxplot(outlier.size = NA,alpha=0,linewidth=2) +\n    geom_point(position = position_jitter(width = .2),pch=21,fill='gray',size=3) +\n    scale_x_discrete(\n        labels = function(x)paste0(as.double(x)*100,\"%\")\n    ) +\n    labs(x = \"Sample Size for MCCV Training\",y=\"AUROC\",caption=paste0(\n        \"As we increase our sample size for learning,\\n\",\n        \"performance increases as expected,\\n\",\n        \"but so does AUROC variability that shows decreased overall performance\")) +\n    theme_bw(base_size = 16)\n```\n\n::: {.cell-output-display}\n![](mccv_power_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nIn short, my thinking is the data generating process is captured in a sample only if a robust signal is found. A robust signal can be represented by a linear, average increase in AUROC performance as sample size using for training increases. Otherwise, the signal-to-noise ratio is lower than what would be needed to make generalizable predictions from the specified model and data. In this last example, the evidence is unclear as expected whether the two classes of data are generated by the same process. I say this for two reasons:\n\n1.  There is stagnant performance between using 30% and 80% of the sample size for training\n2.  There is a stark difference between using 20% and 90% of the sample size for training. I would expect there to be more overlap compared to complete non-overlap.\n\nFeel free to comment on the post for a discussion on my thinking - would love to discuss this!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}